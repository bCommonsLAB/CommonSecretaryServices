From PDF to Markdown with Local LLMs — Fast, Private, and Free
No-Cost, Accurate OCR — Pain-Free, Efficient, and Fully Yours
Gwang-Jin
Gwang-Jin

Follow
8 min read
·
Apr 6, 2025
1K


27






Photo by Mahrous Houses on Unsplash
Ever had a PDF that looked like it was daring you to extract its contents?

You know the kind: lots of embedded images, random formatting, maybe even some scanned pages thrown in for good measure. You try to copy-paste, and what do you get? Garbled gibberish and a headache.

Well, we don’t do that anymore.

Today, I’ll show you how to convert any PDF to clean, structured Markdown using a local LLM (Gemma 3 via Ollama) — no cloud APIs, no privacy worries, no API tokens vanishing at midnight. Just Python, a bit of muscle, and a whole lot of elegance.

Let’s break it down, step by step.

[Free link to this article]

[github]

What We’re Doing
The idea is simple:

1. Turn each PDF page into an image.

2. Send those images to a local LLM using Ollama, specifically gemma3:12b (gemma3:4b works too).

3. Ask the model to extract the readable content in Markdown format.

4. Save the result as a .md file you can actually use.

Now, here’s the twist: it works on scanned PDFs too, because we’re using image input. That’s your OCR, layout detection, and formatting all-in-one.

Tools We Use
PyMuPDF (aka fitz): to render PDF pages as images
Pillow: to convert and store images as PNG bytes
ollama: to chat with a local model (no OpenAI keys needed!)
gemma3:12b (or gemma3:4b): a powerful, privacy-respecting model that can see
Install everything you need:

# install newest uv in macos or linux
# with curl:
curl -LsSf https://astral.sh/uv/install.sh | sh
# or with wget:
wget -qO- https://astral.sh/uv/install.sh | sh
# or in windows:
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"


# create an uv environment
uv init pdftomd
cd pdftomd

# add to the environment the required packages:
uv pip install pymupdf pillow ollama
Make sure you have Ollama installed (otherwise follow instructions in https://ollama.com/download)and running locally, and that gemma3 is pulled:

ollama run gemma3:12b

# alternatively if you don't want to use too much ressources:
ollama run gemma3:4b
The Code

To run the code in a Python shell which uses the defined uv environment, you do either:

# interactively
uv run python   # or uv run ipython if you installed ipython
And later, if you have a running script, you can do:

# from inside the pdftomd folder:
uv run myscript.py
Let’s start the code session with:

import fitz  # PyMuPDF for PDFs
import ollama
import io
from PIL import Image
We import the essentials. PyMuPDF (via fitz) is great for page rendering, and Pillow helps convert raw data into proper PNGs.

Step 1: Convert PDF Pages to Images

def convert_pdf_to_images(pdf_path):
    images = []
    doc = fitz.open(pdf_path)  # Open the PDF
    for page_num in range(len(doc)):
        pix = doc[page_num].get_pixmap()  # Render page to pixel map
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)  # Convert to PIL image
        img_buffer = io.BytesIO()
        img.save(img_buffer, format="PNG")  # Save as in-memory PNG
        images.append(img_buffer.getvalue())  # Raw PNG bytes
    return images
This function reads the PDF, converts each page into a high-res image, and stores them in memory as raw PNG bytes — perfect for sending to an LLM that accepts images.

Why use raw bytes? Because Ollama supports them directly. No need to write files to disk, which is faster and cleaner.

Step 2: Ask the LLM to Extract Text

prompt = "Extract all readable text from these images and format it as structured Markdown."
def query_llm_with_images(image_bytes_list, model="gemma3:12b", prompt=prompt):
    response = ollama.chat(
        model=model,
        messages=[{
            "role": "user",
            "content": prompt,
            "images": image_bytes_list
        }]
    )
    return response["message"]["content"]
This is where the magic happens. You’re sending the image data to your local Gemma model and asking it to do the heavy lifting.

Bonus: Since everything runs locally, your data never leaves your machine. Great for sensitive documents.

Step 3: Putting It All Together

pdf_path = "mypdf.pdf"  # Replace with your PDF file
images = convert_pdf_to_images(pdf_path)

if images:
    print(f"Converted {len(images)} pages to images.")
    
    extracted_text = query_llm_with_images(images)
    
    with open("output.md", "w", encoding="utf-8") as md_file:
        md_file.write(extracted_text)
    print("\nMarkdown Conversion Complete! Check `output.md`.")
else:
    print("No images found in the PDF.")
This final section ties it all together.

We load your PDF.
Convert it to images.
Feed it to the model.
Save the result to output.md.
All in one go.

What You Gain
✅ Markdown-ready output, perfect for LLM pipelines, knowledge bases, or just human readability.
✅ Scanned PDF compatibility, thanks to the image-based approach.
✅ Private by default, since all inference runs locally.
✅ Elegant simplicity — no bloated OCR pipelines or brittle PDF parsers.
Use Cases to Inspire You

Turn old scanned textbooks into Markdown for fine-tuning models
Build an offline document QA system using local embeddings
Feed converted documents into a retrieval-augmented chatbot
Summarize meeting notes, scientific papers, or financial reports
Create a Markdown knowledge base from PDFs, Word files, etc.
Final Thoughts
We often chase complexity because we think that’s where the power lies. But sometimes, it’s about removing friction and making things just click.

This workflow is fast, local, and smart. It gives you Markdown from virtually any PDF with minimal effort, and it runs in a few seconds with zero cloud dependencies.

The full code:

import fitz  # PyMuPDF for PDFs
import ollama
import io
from PIL import Image

def convert_pdf_to_images(pdf_path):
    images = []
    doc = fitz.open(pdf_path)  # Open the PDF
    for page_num in range(len(doc)):
        pix = doc[page_num].get_pixmap()  # Render page to pixel map
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)  # Convert to PIL image
        img_buffer = io.BytesIO()
        img.save(img_buffer, format="PNG")  # Save as in-memory PNG
        images.append(img_buffer.getvalue())  # Raw PNG bytes
    return images

prompt = "Extract all readable text from these images and format it as structured Markdown."
def query_llm_with_images(image_bytes_list, model="gemma3:12b", prompt=prompt):
    response = ollama.chat(
        model=model,
        messages=[{
            "role": "user",
            "content": prompt,
            "images": image_bytes_list
        }]
    )
    return response["message"]["content"]

if __name__ == '__main__':

    pdf_path = "mypdf.pdf"  # Replace with your PDF file
    images = convert_pdf_to_images(pdf_path)

    if images:
        print(f"Converted {len(images)} pages to images.")
    
        extracted_text = query_llm_with_images(images)
    
        with open("output.md", "w", encoding="utf-8") as md_file:
            md_file.write(extracted_text)
        print("\nMarkdown Conversion Complete! Check `output.md`.")
    else:
        print("No images found in the PDF.")
Save this code under nano pdftomd.py and you can run this with:

uv run pdftomd.py
You can extract text from a single image with:

import ollama
import base64

def image_to_text(image_path, model="gemma3:12b", prompt="Extract text from this image"):
    with(open(image_path, "rb") as f:
        image_data = f.read()
    response = ollama.chat(model=model,
                           messages=[{"role": "user",
                                      "content": prompt,
                                      "images": [image_data]}])
    return response["message"]["content"]
And you can improve your results with a better prompt:

prompt = "Extract all readable text and text chunks from this image" + \
         " and format it as structured Markdown." + \
         " Look in the entire image always and try to retrieve all text!"
By changing the prompt and asking for another format (e.g. JSON) or asking for other aspects, you could completely change the behavior of this program. That’s the beauty of programs using LLM prompts. (You could e.g. also ask for a translation of the content “And please translate the extracted content to Korean.”).

How are you extracting Markdown from PDF files or images so far? What experiences did you make with extractions or PDF to Markdown conversions?

Epilogue 1: Using Poppler Instead of PyMuPDF
Medium member 
Milan Agatonovic
 pointed out correctly that PyMuPDF in the fitz package is not free for commercial use. Thank you, 
Milan Agatonovic
!

In the Julia solution equivalent to this article (see below), I used Poppler_jll . We can also use poppler in Python.

Prepare with:

pip install pdf2image
brew install poppler # or sudo apt install poppler-utils
And let’s replace the convert_pdf_to_image() function by:

from pdf2image import convert_from_path
from io import BytesIO

def convert_pdf_to_images(pdf_path):
    images = []
    pil_images = convert_from_path(pdf_path, dpi=300)  # Render each page using Poppler

    for img in pil_images:
        img_buffer = BytesIO()
        img.save(img_buffer, format="PNG")  # Convert PIL image to PNG in memory
        images.append(img_buffer.getvalue())  # Raw PNG bytes

    return images
pdf2image is MIT licensed and depends on Poppler.

Poppler is under GPL (GNU General Public License — you can use it in commercial products but if you statically link or embed it into your app, your entire app must be GPL) — but because you use it as a CLI tool or through shared libs without linking it into your app, you’re generally in the clear for commercial use in this case (see Poppler’s Debian packaging guidelines).

To go 100% license-safe, we could call pdftoppm via subprocess (no pdf2image needed):

import os
import tempfile
import subprocess
from pathlib import Path

def convert_pdf_to_images(pdf_path, dpi=300):
    with tempfile.TemporaryDirectory() as tmpdir:
        output_prefix = os.path.join(tmpdir, "page")

        # The Poppler's pdftoppm call
        subprocess.run([
            "pdftoppm",
            "-png",
            "-r", str(dpi),
            pdf_path,
            output_prefix
        ], check=True)

        # Collect all generated PNG images as byte arrays
        png_files = sorted(Path(tmpdir).glob("*.png"))
        images_bytes = [Path(p).read_bytes() for p in png_files]

        return images_bytes
Epilogue 2: Why Page-by-Page Is Smarter
When you process all pages at once:

You overload the model’s context window especially for its output.
You risk cut-off outputs or complete failure
You waste compute — one mistake ruins everything
Instead, process one page at a time, like a disciplined note-taker.

Here is the improved code for page by page extraction:

import fitz  # PyMuPDF for PDFs
import ollama
import io
from PIL import Image

def convert_pdf_to_images(pdf_path):
    """Yield one PNG byte stream per page — no huge list, no memory bomb."""
    doc = fitz.open(pdf_path)
    for page_num in range(len(doc)):
        pix = doc[page_num].get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        yield page_num + 1, buf.getvalue()

def query_llm_with_image(image_bytes, model="gemma3:12b", prompt=None):
    if prompt is None:
        prompt = (
            "Extract all readable text from this image and format it as structured Markdown."
        )
    response = ollama.chat(
        model=model,
        messages=[{
            "role": "user",
            "content": prompt,
            "images": [image_bytes]
        }]
    )
    return response["message"]["content"]

def extract_pdf_to_markdown(pdf_path, output_file="output.md", prompt=None):
    with open(output_file, "w", encoding="utf-8") as md_file:
        for page_number, image_bytes in convert_pdf_to_images(pdf_path):
            print(f"Processing page {page_number}...")
            try:
                markdown = query_llm_with_image(image_bytes, prompt=prompt)
                md_file.write(f"\n\n## Page {page_number}\n\n")
                md_file.write(markdown)
            except Exception as e:
                print(f"Error processing page {page_number}: {e}")
                md_file.write(f"\n\n## Page {page_number} (Error)\n\n")
                md_file.write(f"_Error extracting content from this page._")

    print(f"\n Done! Markdown saved to {output_file}")

# Usage
if __name__ == '__main__':

    pdf_path = "mypdf.pdf"

    out_path = "output.md"

    prompt = "Extract all readable text and text chunks from this image" + \
             " and format it as structured Markdown." + \
             " Look in the entire image always and try to retrieve all text!"
    
    extract_pdf_to_markdown(pdf_path, output_file=out_path, prompt=prompt)
Why Use gemma3:12b or gemma3:4b?
These models were chosen just as examples — not because they’re perfect, but because:

They fit on most modern laptops or PCs without frying your GPU
They support image input locally via Ollama
They’re a good starting point for experimenting
But let’s be honest:

They’re small. Especially compared to bleeding-edge multimodal giants.

For best results use the best/biggest models your local devices allow.